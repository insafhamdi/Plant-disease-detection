{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"/kaggle/input/plant-leave-diseases-dataset-with-augmentation\"\n",
    "\n",
    "# Define transformations, including scaling down image pixel values\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x / 256)  # Normalizing the images\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Splitting the dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Custom DataLoader\n",
    "def custom_data_loader(dataset, batch_size, shuffle=True):\n",
    "    indices = list(range(len(dataset)))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(dataset), batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        batch = [dataset[i] for i in batch_indices]\n",
    "        images, labels = zip(*batch)\n",
    "        yield torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "# Display function for datasets\n",
    "def show_images(images, labels, predictions=None):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(min(len(images), 5)):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
    "        title = f\"Label: {labels[i]}\"\n",
    "        if predictions is not None:\n",
    "            title += f\"\\nPred: {predictions[i]}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Define the CNN model\n",
    "class MultiClassCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassCNN, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(16 * 127 * 127, 39)  # Adjust according to the number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initializing the model, loss function, and optimizer\n",
    "model = MultiClassCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(train_loader, model, criterion, optimizer, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, data_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = custom_data_loader(train_dataset, batch_size=32)\n",
    "test_loader = custom_data_loader(test_dataset, batch_size=32)\n",
    "\n",
    "# Training the model\n",
    "train_model(train_loader, model, criterion, optimizer)\n",
    "\n",
    "# Evaluating the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "dataset_path = 'C:/Users/Babacar Gaye/Desktop/mes docs babs/mes etudes/ESSAI 2EM ANNE/PFA/Plant_leave_diseases_dataset_with_augmentation'\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([ transforms.Resize((256, 256)),transforms.ToTensor(),          \n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split ratio (e.g., 80% for training, 20% for testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "# Use the Subset class to split the dataset\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader instances for training and testing\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to display images from a DataLoader\n",
    "def show_images(dataloader, num_images=5):\n",
    "    # Get a batch of images\n",
    "    for images, labels in dataloader:\n",
    "        # Plot the images\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(1, num_images, i + 1)\n",
    "            plt.imshow(np.transpose(images[i], (1, 2, 0)))  # Transpose the image tensor to (height, width, channels)\n",
    "            plt.title(f\"Label: {labels[i]}\")\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "        break  # Break after showing one batch\n",
    "\n",
    "# Display images from the training DataLoader\n",
    "show_images(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenez l'ensemble de données parent de votre sous-ensemble de données\n",
    "parent_dataset = train_data.dataset\n",
    "\n",
    "# Obtenez le nombre de classes à partir de l'attribut 'classes' de l'ensemble de données parent\n",
    "num_classes = len(parent_dataset.classes)\n",
    "print(\"Nombre de classes :\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassCNN,self).__init__()\n",
    "        self.conv_block = nn.Sequential(nn.Conv2d(3,16,kernel_size=3, stride=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(16*127*127, 39)\n",
    "    def forward(self,x):\n",
    "        x=self.conv_block(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du modèle\n",
    "model = MultiClassCNN()\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Remise à zéro des gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passage avant (forward pass) pour obtenir les prédictions du modèle\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calcul de la perte\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Rétropropagation des gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Mise à jour des poids du modèle\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "# Mettez le modèle en mode d'évaluation\n",
    "model.eval()\n",
    "\n",
    "# Boucle à travers les données de test\n",
    "for images, labels in test_data:\n",
    "    # Passez les images à travers le modèle pour obtenir les prédictions\n",
    "    with torch.no_grad():  # Pas de calcul de gradient pendant l'évaluation\n",
    "        outputs = model(images.unsqueeze(0))  # Ajoutez une dimension de lot (batch) supplémentaire\n",
    "    predicted_label = torch.argmax(outputs, dim=1).item()  # Convertit les sorties en étiquettes prédites\n",
    "    predictions.append(predicted_label)  # Ajoutez l'étiquette prédite à la liste des prédictions\n",
    "\n",
    "# Affichez les 10 premières prédictions\n",
    "print(\"First 10 predictions:\", predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize metrics\n",
    "accuracy = Accuracy(task='multiclass',num_classes=39, average='macro')\n",
    "precision = Precision(task='multiclass',num_classes=39, average=None)\n",
    "recall = Recall(task='multiclass',num_classes=39, average=None)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate over the test data\n",
    "for images, labels in test_loader:\n",
    "    # Forward pass to get the predicted labels\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Update metrics\n",
    "    accuracy.update(predicted, labels)\n",
    "    precision.update(predicted, labels)\n",
    "    recall.update(predicted, labels)\n",
    "\n",
    "# Compute final values\n",
    "accuracy = accuracy.compute()\n",
    "precision = precision.compute()\n",
    "recall = recall.compute()\n",
    "\n",
    "# Print accuracy, precision, and recall\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Per-class Precision:\", precision)\n",
    "print(\"Per-class Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
